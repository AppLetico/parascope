# Prscope Environment Variables (sample)
# Copy this file to .env and fill in your values.

# =============================================================================
# GitHub (required for upstream sync)
# =============================================================================
GITHUB_TOKEN=github_pat_your_token_here

# =============================================================================
# LLM Providers
# =============================================================================
# Planning mode uses two configurable models (set in prscope.yml):
#   author_model: the model that drafts and refines the plan
#   critic_model: the model that critiques the plan
#
# Both can be any LiteLLM-supported model from any provider.
# You only need API keys for the providers you actually use.
# Example — all-OpenAI setup (no Anthropic key needed):
#   author_model: gpt-4o
#   critic_model: gpt-4o-mini

# OpenAI (gpt-4o, gpt-4o-mini, o1, etc.)
OPENAI_API_KEY=sk-your-openai-key

# Anthropic — OPTIONAL, only needed if critic_model (or author_model) is a Claude model
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key

# Google
GOOGLE_API_KEY=your-google-api-key

# Optional aliases used by some model strings
# GEMINI_API_KEY=your-google-api-key

# Azure OpenAI
# AZURE_API_KEY=your-azure-key
# AZURE_API_BASE=https://your-resource.openai.azure.com
# AZURE_API_VERSION=2024-02-15-preview

# AWS Bedrock
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key
# AWS_REGION_NAME=us-east-1

# Ollama uses local runtime and no API key.
